"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[144],{6144:function(n,e,a){a.r(e),e.default='@inproceedings{remaggi19,\nauthor = {Remaggi, Luca and Kim, Hansung and Neidhardt, Annika and Hilton, Adrian and Jackson, Philip},\nyear = {2019},\nmonth = {09},\npages = {},\ntitle = {Perceived quality and spatial impression of room reverberation in VR reproduction from measured images and acoustics}\n}\n\n@article{Chen2017,\n  author    = {Lele Chen and Sudhanshu Srivastava and Zhiyao Duan and Chenliang Xu},\n  title     = {Deep Cross-Modal Audio-Visual Generation},\n  journal   = {CoRR},\n  volume    = {abs/1704.08292},\n  year      = {2017},\n  url       = {http://arxiv.org/abs/1704.08292},\n  archivePrefix = {arXiv},\n  eprint    = {1704.08292},\n  timestamp = {Mon, 13 Aug 2018 16:47:17 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/ChenSDX17.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n@misc{ganin2016,\n      title={Domain-Adversarial Training of Neural Networks}, \n      author={Yaroslav Ganin and Evgeniya Ustinova and Hana Ajakan and Pascal Germain and Hugo Larochelle and Fran\xe7ois Laviolette and Mario Marchand and Victor Lempitsky},\n      year={2016},\n      eprint={1505.07818},\n      archivePrefix={arXiv},\n      primaryClass={stat.ML}\n}\n@article{ATHANASIADIS2020,\ntitle = "Audio–visual domain adaptation using conditional semi-supervised Generative Adversarial Networks",\njournal = "Neurocomputing",\nvolume = "397",\npages = "331 - 344",\nyear = "2020",\nissn = "0925-2312",\ndoi = "https://doi.org/10.1016/j.neucom.2019.09.106",\nurl = "http://www.sciencedirect.com/science/article/pii/S0925231219316170",\nauthor = "Christos Athanasiadis and Enrique Hortal and Stylianos Asteriadis",\nkeywords = "Domain adaptation, Conformal prediction, Generative adversarial, Networks",\nabstract = "Accessing large, manually annotated audio databases in an effort to create robust models for emotion recognition is a notably difficult task, handicapped by the annotation cost and label ambiguities. On the contrary, there are plenty of publicly available datasets for emotion recognition which are based on facial expressivity due to the prevailing role of computer vision in deep learning research, nowadays. Thereby, in the current work, we performed a study on cross-modal transfer knowledge between audio and facial modalities within the emotional context. More concretely, we investigated whether facial information from videos could be used to boost the awareness and the prediction tracking of emotions in audio signals. Our approach was based on a simple hypothesis: that the emotional state’s content of a person’s oral expression correlates with the corresponding facial expressions. Research in the domain of cognitive psychology was affirmative to our hypothesis and suggests that visual information related to emotions fused with the auditory signal is used from humans in a cross-modal integration schema to better understand emotions. In this regard, a method called dacssGAN (which stands for Domain Adaptation Conditional Semi-Supervised Generative Adversarial Networks) is introduced in this work, in an effort to bridge these two inherently different domains. Given as input the source domain (visual data) and some conditional information that is based on inductive conformal prediction, the proposed architecture generates data distributions that are as close as possible to the target domain (audio data). Through experimentation, it is shown that classification performance of an expanded dataset using real audio enhanced with generated samples produced using dacssGAN (50.29% and 48.65%) outperforms the one obtained merely using real audio samples (49.34% and 46.90%) for two publicly available audio–visual emotion datasets."\n}\n@misc{motiian2017,\n      title={Unified Deep Supervised Domain Adaptation and Generalization}, \n      author={Saeid Motiian and Marco Piccirilli and Donald A. Adjeroh and Gianfranco Doretto},\n      year={2017},\n      eprint={1709.10190},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n@article{Kon2020,\n  title={An auditory scaling method for reverb synthesis from a single two-dimensional image},\n  author={Homare Kon and Hideki Koike},\n  journal={Acoustical Science and Technology},\n  volume={41},\n  number={4},\n  pages={675-685},\n  year={2020},\n  doi={10.1250/ast.41.675}\n}\n@article{Murez2017,\n  author    = {Zak Murez and\n               Soheil Kolouri and\n               David J. Kriegman and\n               Ravi Ramamoorthi and\n               Kyungnam Kim},\n  title     = {Image to Image Translation for Domain Adaptation},\n  journal   = {CoRR},\n  volume    = {abs/1712.00479},\n  year      = {2017},\n  url       = {http://arxiv.org/abs/1712.00479},\n  archivePrefix = {arXiv},\n  eprint    = {1712.00479},\n  timestamp = {Mon, 13 Aug 2018 16:47:07 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1712-00479.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n@article{Kon2019,\nauthor = {Kon, Homare and Koike, Hideki},\nyear = {2019},\nmonth = {08},\npages = {540-548},\ntitle = {Estimation of Late Reverberation Characteristics from a Single Two-Dimensional Environmental Image Using Convolutional Neural Networks},\nvolume = {67},\njournal = {Journal of the Audio Engineering Society},\ndoi = {10.17743/jaes.2018.0069}\n}\n\n@inproceedings{Kon2018,\nauthor = {Kon, Homare and Koike, Hideki},\nyear = {2018},\nmonth = {05},\npages = {},\nbooktitle = {Audio Engineering Society},\ntitle = {Deep neural networks for cross-modal estimations of acoustic reverberation characteristics from two-dimensional images}\n}\n\n@misc{bryan2019,\n    title={Impulse Response Data Augmentation and Deep Neural Networks for Blind Room Acoustic Parameter Estimation},\n    author={Nicholas J. Bryan},\n    year={2019},\n    eprint={1909.03642},\n    archivePrefix={arXiv},\n    primaryClass={cs.SD}\n}\n\n@inproceedings{Oord2016,\ntitle	= {WaveNet: A Generative Model for Raw Audio},\nauthor	= {A\xe4ron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alexander Graves and Nal Kalchbrenner and Andrew Senior and Koray Kavukcuoglu},\nyear	= {2016},\nURL	= {https://arxiv.org/abs/1609.03499},\nbooktitle	= {Arxiv}\n}\n\n\n@article{Zhou2014,\nabstract = {Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers\' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.},\nauthor = {Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},\nfile = {:Users/jeff/Downloads/places{\\_}NIPS14.pdf:pdf},\nissn = {10495258},\njournal = {Advances in Neural Information Processing Systems},\nnumber = {January},\npages = {487--495},\ntitle = {{Learning deep features for scene recognition using places database}},\nvolume = {1},\nyear = {2014}\n}\n\n@inproceedings{lee2010approximating,\n  title={Approximating measured reverberation using a hybrid fixed/switched convolution structure},\n  author={Lee, Keun Sup and Bryan, Nicholas J and Abel, Jonathan S},\n  booktitle={Proceedings of the 13th International Conference on Digital Audio Effects (DAFx’10)},\n  year={2010}\n}\n\n@inproceedings{eaton2015ace,\n  title={The ACE challenge—Corpus description and performance evaluation},\n  author={Eaton, James and Gaubitch, Nikolay D and Moore, Alastair H and Naylor, Patrick A},\n  booktitle={2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},\n  pages={1--5},\n  year={2015},\n  organization={IEEE}\n}\n\n@article{Shorten2019,\nauthor = {Shorten, Connor and Khoshgoftaar, Taghi M.},\ndoi = {10.1186/s40537-019-0197-0},\nfile = {:Users/jeff/Downloads/Shorten-Khoshgoftaar2019{\\_}Article{\\_}ASurveyOnImageDataAugmentation.pdf:pdf},\nissn = {21961115},\njournal = {Journal of Big Data},\nkeywords = {Big data,Data Augmentation,Deep Learning,GANs,Image data},\nnumber = {1},\npublisher = {Springer International Publishing},\ntitle = {{A survey on Image Data Augmentation for Deep Learning}},\nurl = {https://doi.org/10.1186/s40537-019-0197-0},\nvolume = {6},\nyear = {2019}\n}\n\n@article{Gan2020,\narchivePrefix = {arXiv},\narxivId = {2007.04954},\nauthor = {Gan, Chuang and Schwartz, Jeremy and Alter, Seth and Schrimpf, Martin and Traer, James and {De Freitas}, Julian and Kubilius, Jonas and Bhandwaldar, Abhishek and Haber, Nick and Sano, Megumi and Kim, Kuno and Wang, Elias and Mrowca, Damian and Lingelbach, Michael and Curtis, Aidan and Feigelis, Kevin and Bear, Daniel M. and Gutfreund, Dan and Cox, David and DiCarlo, James J. and McDermott, Josh and Tenenbaum, Joshua B. and Yamins, Daniel L. K.},\neprint = {2007.04954},\nfile = {:Users/jeff/Downloads/2007.04954.pdf:pdf},\ntitle = {{ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation}},\nurl = {http://arxiv.org/abs/2007.04954},\nyear = {2020}\n}\n\n@article{Schroeder1961,\nabstract = {Electronic devices are widely used to introduce in sound signals an artificial reverberation subjectively similar to that caused by multiple reflections in a room. Attention is focused on those devices employing delay loops. Usually, these devices have a comb-like frequency response which adds an undesired “color” to the sound quality. Also, for a given reverberation time, the density of echoes is far below that encountered in a room, giving rise to a noticeable flutter effect in transient sounds. A class of all-pass filters is described which may be employed in cascade to obtain “colorless” reverberation with high echo density. COPYRIGHT {\\textcopyright} 1962—THE INSTITUTE OF RADIO ENGINEERS, INC.},\nauthor = {Schroeder, M. R. and Logan, B. F.},\ndoi = {10.1109/TAU.1961.1166351},\nfile = {:Users/jeff/Downloads/Schroeder{\\_}1961.pdf:pdf},\nissn = {21682984},\njournal = {IRE Transactions on Audio},\nnumber = {6},\npages = {209--214},\ntitle = {{“Colorless” Artificial Reverberation}},\nvolume = {9},\nyear = {1961}\n}\n\n@inproceedings{engel2018gansynth,\ntitle={{GANS}ynth: Adversarial Neural Audio Synthesis},\nauthor={Jesse Engel and Kumar Krishna Agrawal and Shuo Chen and Ishaan Gulrajani and Chris Donahue and Adam Roberts},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xQVn09FX},\n}\n\n@inproceedings{donahue2018adversarial,\ntitle={Adversarial Audio Synthesis},\nauthor={Chris Donahue and Julian McAuley and Miller Puckette},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ByMVTsR5KQ},\n}\n\n@inproceedings{antsalo2001estimation,\n  title={Estimation of modal decay parameters from noisy response measurements},\n  author={Antsalo, Poju and Makivirta, Aki and Valimaki, Vesa and Peltonen, Timo and Karjalainen, Matti},\n  booktitle={Audio Engineering Society Convention 110},\n  year={2001},\n  organization={Audio Engineering Society}\n}\n\n@inproceedings{he2016deep,\n  title={Deep Residual Learning for Image Recognition},\n  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages={770--778},\n  year={2016}\n}\n\n@article{griffin1984signal,\n  title={Signal estimation from modified short-time Fourier transform},\n  author={Griffin, Daniel and Lim, Jae},\n  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing},\n  volume={32},\n  number={2},\n  pages={236--243},\n  year={1984},\n  publisher={IEEE}\n}\n\n@inproceedings{ananthabhotla2019towards,\n  title={Towards a Perceptual Loss: Using a Neural Network Codec Approximation as a Loss for Generative Audio Models},\n  author={Ananthabhotla, Ishwarya and Ewert, Sebastian and Paradiso, Joseph A},\n  booktitle={Proceedings of the 27th ACM International Conference on Multimedia},\n  pages={1518--1525},\n  year={2019}\n}\n\n@inproceedings{remaggi2019reproducing,\n  title={Reproducing real world acoustics in virtual reality using spherical cameras},\n  author={Remaggi, Luca and Kim, Hansung and Jackson, Philip JB and Hilton, Adrian},\n  booktitle={Proceedings of the 2019 AES International Conference on Immersive and Interactive Audio},\n  year={2019},\n  organization={Audio Engineering Society}\n}\n\n@article{lindau2012assessing,\n  title={Assessing the plausibility of virtual acoustic environments},\n  author={Lindau, Alexander and Weinzierl, Stefan},\n  journal={Acta Acustica united with Acustica},\n  volume={98},\n  number={5},\n  pages={804--810},\n  year={2012},\n  publisher={S. Hirzel Verlag}\n}\n\n@misc{devries2020on,\ntitle={On the Evaluation of Conditional {\\{}GAN{\\}}s},\nauthor={Terrance DeVries and Adriana Romero and Luis Pineda and Graham W. Taylor and Michal Drozdzal},\nyear={2020},\nurl={https://openreview.net/forum?id=rylxpA4YwH}\n}\n\n@misc{sali2020,\ntitle={Generating Impulse Responses using\nRecurrent Neural Networks},\nauthor={Kaushal Sali and Alexander Lerch},\nyear={2020},\nurl={https://109ecc9c-0e76-482f-90c5-fe6cd93cf581.filesusr.com/ugd/4a27c6_fa8281568425494e8ca16133fe724c6e.pdf}\n}\n\n@misc{steinmetz2018,\ntitle={NeuralReverberator},\nauthor={Christian Steinmetz},\nyear={2018},\nurl={https://www.christiansteinmetz.com/projects-blog/neuralreverberator}\n}\n\n@inproceedings{bryan2020impulse,\n  title={Impulse Response Data Augmentation and Deep Neural Networks for Blind Room Acoustic Parameter Estimation},\n  author={Bryan, Nicholas J},\n  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={1--5},\n  year={2020},\n  organization={IEEE}\n}\n\n@article{rettinger1957reverberation,\n  title={Reverberation chambers for broadcasting and recording studios},\n  author={Rettinger, Michael},\n  journal={Journal of the Audio Engineering Society},\n  volume={5},\n  number={1},\n  pages={18--22},\n  year={1957},\n  publisher={Audio Engineering Society}\n}\n\n@inproceedings{reilly1995convolution,\n  title={Convolution processing for realistic reverberation},\n  author={Reilly, Andrew and McGrath, David},\n  booktitle={Audio Engineering Society Convention 98},\n  year={1995},\n  organization={Audio Engineering Society}\n}\n\n@article{robjohns1999sony,\n  title={Sony DRE S777 sampling digital reverb},\n  author={Robjohns, H},\n  journal={Sound on Sound},\n  volume={15},\n  year={1999}\n}\n\n@inproceedings{anderegg2004convolution,\n  title={Implementation of High-Order Convolution Algorithms with Low Latency on Silicon Chips},\n  author={Anderegg, Rolf and Felber, Norbert and Fichtner, Wolfgang and Franke, Ulrich},\n  booktitle={Audio Engineering Society Convention 117},\n  year={2004},\n  organization={Audio Engineering Society}\n}\n\n@article{valimaki2012fifty,\n  title={Fifty years of artificial reverberation},\n  author={Valimaki, Vesa and Parker, Julian D and Savioja, Lauri and Smith, Julius O and Abel, Jonathan S},\n  journal={IEEE Transactions on Audio, Speech, and Language Processing},\n  volume={20},\n  number={5},\n  pages={1421--1448},\n  year={2012},\n  publisher={IEEE}\n}\n\n@inproceedings{goodfellow2014generative,\n  title={Generative adversarial nets},\n  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},\n  booktitle={Advances in neural information processing systems},\n  pages={2672--2680},\n  year={2014}\n}\n\n@article{mirza2014conditional,\n  title={Conditional generative adversarial nets},\n  author={Mirza, Mehdi and Osindero, Simon},\n  journal={arXiv preprint arXiv:1411.1784},\n  year={2014}\n}\n\n@misc{gui2020review,\n      title={A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications}, \n      author={Jie Gui and Zhenan Sun and Yonggang Wen and Dacheng Tao and Jieping Ye},\n      year={2020},\n      eprint={2001.06937},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n\n@article{davis1980comparison,\n  title={Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences},\n  author={Davis, Steven and Mermelstein, Paul},\n  journal={IEEE transactions on acoustics, speech, and signal processing},\n  volume={28},\n  number={4},\n  pages={357--366},\n  year={1980},\n  publisher={IEEE}\n}\n\n@article{schroeder1965new,\n  title={New method of measuring reverberation time},\n  author={Schroeder, Manfred R},\n  journal={The Journal of the Acoustical Society of America},\n  volume={37},\n  number={6},\n  pages={1187--1188},\n  year={1965},\n  publisher={Acoustical Society of America}\n}\n\n@article{jordan1970acoustical,\n  title={Acoustical criteria for auditoriums and their relation to model techniques},\n  author={Jordan, Vilhelm Lassen},\n  journal={The Journal of the Acoustical Society of America},\n  volume={47},\n  number={2A},\n  pages={408--412},\n  year={1970},\n  publisher={Acoustical Society of America}\n}\n\n@inproceedings{jo1975measurement,\n  title={Measurement of reverberation time based on the direct-reverberant sound energy ratio in steady state},\n  author={Jo, Touju and Koyasu, Masaru},\n  booktitle={INTER-NOISE and NOISE-CON Congress and Conference Proceedings},\n  volume={1975},\n  number={2},\n  pages={579--582},\n  year={1975},\n  organization={Institute of Noise Control Engineering}\n}\n\n@INPROCEEDINGS{jndt60,  author={Z. {Meng} and F. {Zhao} and M. {He}},  booktitle={2006 International Symposium on Communications and Information Technologies},   title={The Just Noticeable Difference of Noise Length and Reverberation Perception},   year={2006},  volume={},  number={},  pages={418-421},  doi={10.1109/ISCIT.2006.339980}}\n\n@book{sabine1975acoustical,\n  title={Acoustical and thermal performance of exterior residential walls, doors, and windows},\n  author={Sabine, Hale J},\n  volume={77},\n  year={1975},\n  publisher={US Department of Commerce, National Bureau of Standards}\n}\n\n@article{brown1964acoustic,\n  title={Acoustic design of broadcasting studios},\n  author={Brown, Sandy},\n  journal={Journal of Sound and Vibration},\n  volume={1},\n  number={3},\n  pages={239--257},\n  year={1964},\n  publisher={Elsevier}\n}\n\n@article{kingma2014adam,\n  title={Adam: A method for stochastic optimization},\n  author={Kingma, Diederik P and Ba, Jimmy},\n  journal={arXiv preprint arXiv:1412.6980},\n  year={2014}\n}\n\n@inproceedings{murphy2010openair,\n  title={Openair: An interactive auralization web resource and database},\n  author={Murphy, Damian T and Shelley, Simon},\n  booktitle={Audio Engineering Society Convention 129},\n  year={2010},\n  organization={Audio Engineering Society}\n}\n\n@inproceedings{pytorch2019,\n author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\\textquotesingle Alch\\\'{e}-Buc and E. Fox and R. Garnett},\n pages = {8026--8037},\n publisher = {Curran Associates, Inc.},\n title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},\n url = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},\n volume = {32},\n year = {2019}\n}\n\n@article{mentzer2020high,\n  title={High-Fidelity Generative Image Compression},\n  author={Mentzer, Fabian and Toderici, George D and Tschannen, Michael and Agustsson, Eirikur},\n  journal={Advances in Neural Information Processing Systems},\n  volume={33},\n  year={2020}\n}\n\n@inproceedings{monodepth2,\n  title     = {Digging into Self-Supervised Monocular Depth Prediction},\n  author    = {Cl{\\\'{e}}ment Godard and\n               Oisin {Mac Aodha} and\n               Michael Firman and\n               Gabriel J. Brostow},\n  booktitle = {The International Conference on Computer Vision (ICCV)},\n  month = {October},\nyear = {2019}\n}\n\n@inproceedings{mao2017least,\n  title={Least squares generative adversarial networks},\n  author={Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond YK and Wang, Zhen and Paul Smolley, Stephen},\n  booktitle={Proceedings of the IEEE international conference on computer vision},\n  pages={2794--2802},\n  year={2017}\n}\n\n@article{traer2016statistics,\n  title={Statistics of natural reverberation enable perceptual separation of sound and space},\n  author={Traer, James and McDermott, Josh H},\n  journal={Proceedings of the National Academy of Sciences},\n  volume={113},\n  number={48},\n  pages={E7856--E7865},\n  year={2016},\n  publisher={National Acad Sciences}\n}\n\n@article{badeau2019common,\n  title={Common mathematical framework for stochastic reverberation models},\n  author={Badeau, Roland},\n  journal={The Journal of the Acoustical Society of America},\n  volume={145},\n  number={4},\n  pages={2733--2745},\n  year={2019},\n  publisher={Acoustical Society of America}\n}\n\n@inproceedings{selvaraju2017grad,\n  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},\n  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},\n  booktitle={Proceedings of the IEEE international conference on computer vision},\n  pages={618--626},\n  year={2017}\n}\n\n@article{wang2004image,\n  title={Image quality assessment: from error visibility to structural similarity},\n  author={Wang, Zhou and Bovik, Alan C and Sheikh, Hamid R and Simoncelli, Eero P},\n  journal={IEEE transactions on image processing},\n  volume={13},\n  number={4},\n  pages={600--612},\n  year={2004},\n  publisher={IEEE}\n}\n\n@article{maaten2008visualizing,\n  title={Visualizing data using t-SNE},\n  author={Maaten, Laurens van der and Hinton, Geoffrey},\n  journal={Journal of machine learning research},\n  volume={9},\n  number={Nov},\n  pages={2579--2605},\n  year={2008}\n}\n\n@article{dalle,\n  title={DALL\xb7E: Creating\nImages from Text},\n  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott},\n  journal={OpenAI Blog},\n  year={2021}\n}\n\n@inproceedings{pellegrini2001quality,\n  title={Quality assessment of auditory virtual environments},\n  author={Pellegrini, Renato S},\n  year={2001},\n  booktitle={Proceedings of the 2001 International Conference on Auditory Display}\n}\n\n@inproceedings{karras2018progressive,\n  title={Progressive Growing of GANs for Improved Quality, Stability, and Variation},\n  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},\n  booktitle={International Conference on Learning Representations},\n  year={2018}\n}\n\n@article{Schissler2016,\nauthor = {Schissler, Carl and Manocha, Dinesh},\ntitle = {Interactive Sound Propagation and Rendering for Large Multi-Source Scenes},\nyear = {2016},\nissue_date = {July 2017},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nvolume = {36},\nnumber = {4},\nissn = {0730-0301},\nurl = {https://doi.org/10.1145/3072959.2943779},\ndoi = {10.1145/3072959.2943779},\njournal = {ACM Trans. Graph.},\nmonth = {9},\narticleno = {114c},\nnumpages = {12},\nkeywords = {ray tracing, Sound propagation, source clustering}\n}\n\n@misc{ratnarajah2021,\n      title={IR-GAN: Room Impulse Response Generator for Speech Augmentation}, \n      author={Anton Ratnarajah and Zhenyu Tang and Dinesh Manocha},\n      year={2021},\n      eprint={2010.13219},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD}\n}\n\n@article{pearson1977tests,\n  title={Tests for departure from normality: Comparison of powers},\n  author={Pearson, Egon S and D ‘‘\'AGOSTINO, Ralph B and Bowman, Kimiko O},\n  journal={Biometrika},\n  volume={64},\n  number={2},\n  pages={231--246},\n  year={1977},\n  publisher={Oxford University Press}\n}\n\n@article{greenhouse1959methods,\n  title={On methods in the analysis of profile data},\n  author={Greenhouse, Samuel W and Geisser, Seymour},\n  journal={Psychometrika},\n  volume={24},\n  number={2},\n  pages={95--112},\n  year={1959},\n  publisher={Springer}\n}\n\n@article{li2018,\nauthor = {Li, Dingzeyu and Langlois, Timothy R. and Zheng, Changxi},\ntitle = {Scene-Aware Audio for 360\xb0 Videos},\nyear = {2018},\nissue_date = {August 2018},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nvolume = {37},\nnumber = {4},\nissn = {0730-0301},\nurl = {https://doi.org/10.1145/3197517.3201391},\ndoi = {10.1145/3197517.3201391},\nabstract = {Although 360\xb0 cameras ease the capture of panoramic footage, it remains challenging to add realistic 360\xb0 audio that blends into the captured scene and is synchronized with the camera motion. We present a method for adding scene-aware spatial audio to 360\xb0 videos in typical indoor scenes, using only a conventional mono-channel microphone and a speaker. We observe that the late reverberation of a room\'s impulse response is usually diffuse spatially and directionally. Exploiting this fact, we propose a method that synthesizes the directional impulse response between any source and listening locations by combining a synthesized early reverberation part and a measured late reverberation tail. The early reverberation is simulated using a geometric acoustic simulation and then enhanced using a frequency modulation method to capture room resonances. The late reverberation is extracted from a recorded impulse response, with a carefully chosen time duration that separates out the late reverberation from the early reverberation. In our validations, we show that our synthesized spatial audio matches closely with recordings using ambisonic microphones. Lastly, we demonstrate the strength of our method in several applications.},\njournal = {ACM Trans. Graph.},\nmonth = {7},\narticleno = {111},\nnumpages = {12},\nkeywords = {360\xb0 videos, ambisonic audio}\n}\n\n@inproceedings{gao2019visual-sound,\n  title = {2.5D Visual Sound},\n  author = {Gao, Ruohan and Grauman, Kristen},\n  booktitle = {CVPR},\n  year = {2019}\n}\n\n@article{tang2020scene,\ntitle={Scene-Aware Audio Rendering via Deep Acoustic Analysis},\nauthor={Tang, Zhenyu and Bryan, Nicholas J and Li, Dingzeyu and Langlois, Timothy R and Manocha, Dinesh},\njournal={IEEE Transactions on Visualization and Computer Graphics},\nyear={2020},\npublisher={IEEE}\n}\n\n@INPROCEEDINGS{kim2019, author={H. {Kim} and L. {Remaggi} and P. J. B. {Jackson} and A. {Hilton}},  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},   title={Immersive Spatial Audio Reproduction for VR/AR Using Room Acoustic Modelling from 360\xb0 Images},   year={2019},  volume={},  number={},  pages={120-126},  doi={10.1109/VR.2019.8798247}}'}}]);