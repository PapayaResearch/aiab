(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[585],{6083:function(e,t,n){var a={"./main.bib":[6144,144]};function i(e){if(!n.o(a,e))return Promise.resolve().then(function(){var t=Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t});var t=a[e],i=t[0];return n.e(t[1]).then(function(){return n(i)})}i.keys=function(){return Object.keys(a)},i.id=6083,e.exports=i},9918:function(e,t,n){(window.__NEXT_P=window.__NEXT_P||[]).push(["/tex",function(){return n(264)}])},264:function(e,t,n){"use strict";n.r(t),n.d(t,{__N_SSG:function(){return N},default:function(){return G}});var a=n(5893),i=n(7294),r=n(9236),s=n(1232),o=n(8818),l=n(4685),c=n(5117),d=n(9094),h=n(7414),u=n(966),p=n(9380),m=n(9317),g=n(5001);n(4905);var f=n(7990),b=n(1371);let w=e=>{let t=e.content.filter(e=>{if("environment"===e.type)return"document"===e.env});return t[0]},v=()=>e=>(0,g.Vn)(e,e=>(0,f.KJ)(e),{includeArrays:!0,test:Array.isArray}),y=async e=>{let t=await (0,p.l)().use(m.Q5).use(v).use(m.as).process(e).then(e=>(0,b.H)(e.result)).then(w);return t};var x=n(3619),$="\\documentclass[10pt,twocolumn,letterpaper]{article}\n\n\\usepackage{iccv}\n\\usepackage{times}\n\\usepackage{epsfig}\n\\usepackage{graphicx}\n\\usepackage{float}\n\\usepackage{stfloats}\n\\usepackage{subfigure}\n\\usepackage{multirow}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{cuted}\n\\usepackage{capt-of}\n\\usepackage[ruled]{algorithm2e}\n\n\\usepackage[breaklinks=true,bookmarks=false,colorlinks]{hyperref}\n\n\\iccvfinalcopy\n\n\\def\\iccvPaperID{9135}\n\\def\\httilde{\\mbox{\\tt\\raisebox{-.5ex}{\\symbol{126}}}}\n\n\n\\ificcvfinal\\pagestyle{empty}\\fi\n\n\\begin{document}\n\n\n\\title{Image2Reverb: Cross-Modal Reverb Impulse Response Synthesis}\n\n\\author{Nikhil Singh\\\\\nMIT\\\\\nMedia Lab\\\\\n{\\tt\\small nsingh1@mit.edu}\n\\and\nJeff Mentch\\\\\nHarvard University\\\\\nSHBT\\\\\n{\\tt\\small jsmentch@g.harvard.edu}\n\\and\nJerry Ng\\\\\nMIT\\\\\nMechanical Engineering\\\\\n{\\tt\\small jerryng@mit.edu}\n\\and\nMatthew Beveridge\\\\\nMIT\\\\\nEECS\\\\\n{\\tt\\small mattbev@mit.edu}\n\\and\nIddo Drori\\\\\nMIT\\\\\nEECS\\\\\n{\\tt\\small idrori@mit.edu}\n}\n\n\\maketitle\n\\ificcvfinal\\thispagestyle{empty}\\fi\n\n\\begin{strip}\n    \\centering\n    \\includegraphics[width=\\textwidth]{teaser-clean.png}\n    \\captionof{figure}{An application of our work: matching speaker audio to expected reverberation of a video conference virtual background.\\\\ Project page with examples is available at \\href{https://web.media.mit.edu/~nsingh1/image2reverb}{https://web.media.mit.edu/\\raisebox{0.5ex}{\\texttildelow}nsingh1/image2reverb}.}\n    \\label{fig:t}\n\\end{strip}\n\n\\ificcvfinal\\thispagestyle{empty}\\fi\n\n\\begin{figure*}[!ht]\n    \\centering\n    \\includegraphics[width=\\textwidth]{splash.png}\n    \\caption{Generating audio impulse responses from images. Left: given an image of an acoustic environment as input, our model generates the corresponding audio impulse response as output. Right: generated impulse responses are convolved with an anechoic (free from echo) audio recording making that recording sound as if it were in the corresponding space. Waveforms and spectrograms are shown of the source anechoic signal and the same signal after convolution with the corresponding synthesized IR. All spectrograms are presented on a mel scale. Image2Reverb is the first system demonstrating end-to-end synthesis of realistic IRs from single images.}\n    \\label{fig:room2reverb_banner}\n\\end{figure*}\n\n\\begin{abstract}\nMeasuring the acoustic characteristics of a space is often done by capturing its impulse response (IR), a representation of how a full-range stimulus sound excites it. This work generates an IR from a single image, which can then be applied to other signals using convolution, simulating the reverberant characteristics of the space shown in the image. Recording these IRs is both time-intensive and expensive, and often infeasible for inaccessible locations. We use an end-to-end neural network architecture to generate plausible audio impulse responses from single images of acoustic environments. We evaluate our method both by comparisons to ground truth data and by human expert evaluation. We demonstrate our approach by generating plausible impulse responses from diverse settings and formats including well known places, musical halls, rooms in paintings, images from animations and computer games, synthetic environments generated from text, panoramic images, and video conference backgrounds.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{introduction}\nAn effective and widely used method of simulating acoustic spaces relies on audio impulse responses (IRs) and convolution \\cite{valimaki2012fifty, robjohns1999sony}. Audio IRs are recorded measurements of how an environment responds to an acoustic stimulus. IRs can be measured by recording a space during a burst of white noise like a clap, a balloon pop, or a sinusoid swept across the range of human hearing \\cite{reilly1995convolution}. Accurately capturing these room impulse responses requires time, specialized equipment, knowledge, and planning. Directly recording these measurements may be entirely infeasible in continuously inhabited or inaccessible spaces of interest. End-to-end IR estimation has far ranging applications relevant to fields including music production, speech processing, and generating immersive extended reality environments. Our Image2Reverb system directly synthesizes IRs from images of acoustic environments. This approach removes the barriers to entry, namely cost and time, opening the door for a broad range of applications.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{ir_explanatory.png}\n    \\caption{Impulse response overview. \\textbf{(A)} Sound waves propagate across multiple paths as they interact with and reflect off their environment. These paths include the direct path from source to listener, early reflections including 1st and higher order reflections (after reflecting off 1 or more surfaces) and a more diffuse tail as they trail off and become more densely packed in time. These reflections make up the impulse response of the environment illustrated \\textbf{(B)} schematically and \\textbf{(C)} as a waveform.}\n    \\label{fig:ir_explanatory}\n\\end{figure}\n\nIn this work we model IR generation as a cross-modal paired-example domain adaptation problem and apply a conditional GAN \\cite{goodfellow2014generative, gui2020review, mirza2014conditional} to synthesize plausible audio impulse responses conditioned on images of spaces. Next, we will describe related work that informs our approach.\n\n\\section{Related Work}\n\\label{previouswork}\n\n\\paragraph{Artificial reverberation.}\nHistorically, recording studios built reverberant chambers with speakers and microphones to apply reverb to pre-recorded audio directly within a physical space \\cite{rettinger1957reverberation}. Reverberation circuits, first proposed in the 1960s, use a network of filters and delay lines to mimic a reverberant space \\cite{Schroeder1961}. Later, Digital algorithmic approaches applied numerical methods to simulate similar effects. Conversely, convolution reverb relies on audio recordings of a space's response to a broadband stimulus, typically a noise burst or sine sweep. This results in a digital replica of a space's reverberant characteristics, which can then be applied to any audio signal \\cite{anderegg2004convolution}.\n\nConvolutional neural networks have been used for estimating late-reverberation statistics from images \\cite{Kon2019, Kon2020}, though not to model the full audio impulse response from an image. This work is based on the finding that experienced acoustic engineers readily estimate a space's IR or reverberant characteristics from an image \\cite{Kon2018}. Room geometry has also been estimated from 360-degree images of four specific rooms \\cite{remaggi2019reproducing}, and used to create virtual acoustic environments which are compared with ground-truth recordings, though again IRs are not directly synthesized from the images. A related line of work synthesizes spatial audio based on visual information \\cite{li2018, gao2019visual-sound, kim2019}. Prior work exists on synthesis of IRs using RNNs \\cite{sali2020}, autoencoders \\cite{steinmetz2018}, and GANs: IR-GAN \\cite{ratnarajah2021} uses parameters from real world IRs to generate new synthetic IRs; whereas our work synthesizes an audio impulse response directly from an image.\n\n\n\\paragraph{Generative models for audio.}\nRecent work has shown that GANs are amenable to audio generation and can result in more globally coherent outputs \\cite{donahue2018adversarial}. GANSynth \\cite{engel2018gansynth} generates an audio sequence in parallel via a progressive GAN architecture allowing faster than real-time synthesis and higher efficiency than the autoregressive WaveNet \\cite{Oord2016} architecture. Unlike WaveNet which uses a time-distributed latent coding, GANSynth synthesizes an entire audio segment from a single latent vector. Given our need for global structure, we create a fixed-length representation of our input and adapt our generator model from this approach.\n\nMeasured IRs have been approximated with shaped noise \\cite{lee2010approximating, bryan2020impulse}. While room IRs exhibit statistical regularities \\cite{traer2016statistics} that can be modeled stochastically, the domain of this modeling is time and frequency limited \\cite{badeau2019common}, and may not reflect all characteristics of real-world recorded IRs. Simulating reverb with ray tracing is possible but prohibitively expensive for typical applications \\cite{Schissler2016}. By directly approximating measured audio IRs at the spectrogram level, our outputs are immediately applicable to tasks such as convolution reverb, which applies the reverberant characteristics of the IR to another audio signal.\n\n\\paragraph{Cross-modal translation.}\nBetween visual and auditory domains, conditional GANs have been used for translating between images and audio samples of people playing instruments \\cite{Chen2017}. Our work builds on this by applying state-of-the-art architectural approaches for scene analysis and high quality audio synthesis, tuned for our purposes.\n\n\\section{Methods}\n\\label{methods}\n\\noindent Here we describe the dataset, model, and algorithm.\n\n\\subsection{Dataset}\n\n\\paragraph{Data aggregation.}\nWe curated a dataset of 265 different spaces totalling 1169 images and 738 IRs. From these, we produced a total of 11234 paired examples with a train-validation-test split of 9743-154-1957. These are assembled from sources including the OpenAIR dataset \\cite{murphy2010openair}, other libraries available online, and web scraping. Many examples amount to weak supervision, due to the low availability of data: for example, we may have a ``kitchen\" impulse response without an image of the kitchen in which it was recorded. In this case, we augmented with plausible kitchen scenes, judged by the researchers, gathered via web scraping and manual filtering. Although this dataset contains high variability in several reverberant parameters, e.g. early reflections and source-microphone distance, it allows us to learn characteristics of late-field reverberation.\n\n\\paragraph{Data preprocessing.}\nImages needed to be filtered manually to remove duplicates, mismatches such as external pictures of an indoor space, examples with significant occlusive ``clutter\" or excessive foreground activity, and intrusive watermarks. We then normalized, center-cropped at the max width or height possible, and downsampled to 224x224 pixels. We converted the audio IR files to monaural signals; in the case of Ambisonic B-Format sources we extracted the $W$ (omnidirectional) channel, and for stereo sources we computed the arithmetic mean of channels. In some cases, 360-degree images were available and in these instances we extract rectilinear projections, bringing them in line with the standard 2D images in our dataset.\n\n\\paragraph{Audio representation.}\nOur audio representation is a log magnitude spectrogram. We first resampled the audio files to 22.050kHz and truncate them to 5.94s in duration. This is sufficient to capture general structure and estimate reverberant characteristics for most examples. We then apply a short-time Fourier transform with window size ($M=1024$) and hop size ($R=256$), before trimming the Nyquist bin, resulting in square 512x512 spectrograms. Finally, we take $\\log(\\lvert X \\rvert)$ where $\\lvert X \\rvert$ represents the magnitude spectrogram; audio IRs typically contain uncorrelated phase, which does not offer structure we can replicate based on the magnitude.\n\n\\subsection{Model}\n\n\\paragraph{Components.}\nOur model employs a conditional GAN with an image encoder that takes images as input and produces spectrograms. This overall design, with an encoder, generator, and conditional discriminator, is similar to that which Mentzer et al. \\cite{mentzer2020high} applied to obtain state-of-the-art results on image compression, among many other applications. The generator and discriminator are deep convolutional networks based on the GANSynth \\cite{engel2018gansynth} model (non-progressive variant), with modifications to suit our dataset, dimensions, and training procedure.\n\nThe encoder module combines image feature extraction with depth estimation to produce latent vectors from two-dimensional images of scenes. For depth estimation, we use the pretrained Monodepth2 network \\cite{monodepth2}, a monocular depth-estimation encoder-decoder network which produces a one-channel depth map corresponding to our input image. The main feature extractor is a ResNet50 \\cite{he2016deep} pretrained on Places365 \\cite{Zhou2014} which takes a four-channel representation of our scene including the depth channel (4x224x224). We add randomly initialized weights to accommodate the additional input channel for the depth map. Since we are fine-tuning the entire network, albeit at a low learning rate, we expect it will learn the relevant features during optimization. Our architecture's components are shown in Figure \\ref{fig:arch_summary}.\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\textwidth]{image2reverb_archsummary2.png}\n    \\caption{System architecture. Our system consists of autoencoder and GAN networks. Left: An input image is converted into 4 channels: red, green, blue and depth. The depth map is estimated by Monodepth2, a pre-trained encoder-decoder network. Right: Our model employs a conditional GAN. An image feature encoder is given the RGB and depth images and produces part of the Generator's latent vector which is then concatenated with noise. The Discriminator applies the image latent vector label at an intermediate stage via concatenation to make a conditional real/fake prediction, calculating loss and optimizing the Encoder, Generator, and Discriminator.}\n    \\label{fig:arch_summary}\n\\end{figure*}\n\n\\begin{algorithm}\n    \\caption{Forward and backward passes through the Image2Reverb model. Notation is explained in Table \\ref{tab:vardefinitions}.}\n    \\label{algo:model}\n    \\KwIn{}\n    Monodepth2: $x \\sim X$; Encoder $\\tilde{x} \\sim \\tilde{X}$; Generator: $z = E(\\tilde{x}) \\oplus u$; Discriminator: $(G(z), E(\\tilde{x}))$ OR $(y, E(\\tilde{x}))$;\\\\\n    Parameters: (weight variables);\\\\\n    \\KwOut{}\n    Monodepth2: $\\mathbf{x_d}$; Encoder: $E(\\tilde{x})$; Generator: G(z); Discriminator: $D(G(z), E(\\tilde{x}))$ OR $D(y, E(\\tilde{x}))$; \\\\\n    \\For{number of epochs}{\n    Sample $B$ training images; \\\\\n    Get depth $x_d = M(x)$; \\\\\n    Append depth features to RGB channels ($ y \\oplus {y_d} $); \\\\\n    Encoder image to feature-vector ($E(\\tilde{x})$); \\\\\n    Append noise ($z = E(\\tilde{x}) \\oplus u$);\n    \\\\ Generate spectrogram ($G(z)$);\n    \\\\ Forward pass through discriminator with either fake or real spectrogram ($D(G(z)| E(\\tilde{x}))$ OR $D(y | E(\\tilde{x}))$);\n    \\\\ Backward pass: update parameters for discriminator (${W_D}$), generator (${W_G}$), and encoder (${W_E}$);\n    }\n\\end{algorithm}\n\n\\begin{table}[ht]\n    \\small\n    \\centering\n    \\begin{tabular}{c|c}\n        Notation & Definition \\\\\n        \\hline\n         x & input image \\\\\n         ${x_d}$ & estimated depth map \\\\\n         $\\oplus$ & concatenation operator\\\\\n         $\\tilde{x}$ & image with depth map ($x \\oplus {x_d}$) \\\\\n         $y$ & Real spectrogram \\\\\n         $E, G, D$ & Encoder, Generator, Discriminator\\\\\n         $M$ & Monodepth2 Encoder-Decoder \\\\\n         ${W_*}$ & weights for a model \\\\\n         $u$ & Noise, $u \\sim \\mathcal{N}(0, 1) $\\\\\n         $z$ & Latent vector, encoder output and noise\\\\ & ($E(\\tilde{x}) \\oplus u$) \\\\\n    \\end{tabular}\n    \\caption{Notation and definitions for variables indicated in different parts of this paper.}\n    \\label{tab:vardefinitions}\n\\end{table}\n\n\\paragraph{Objectives.}\nWe use the least-squares GAN formulation (LSGAN) \\cite{mao2017least}. For the discriminator:\n\n\\begin{align}\n\\begin{split}\n    \\underset{D}{\\text{min}}\\ V(D) &  = \\mathbb{E}_{\\mathbf{y} \\sim p_{data}(\\mathbf{y})}[(1 - D(y\\ |\\ E(\\tilde{x}))^2]\\\\\n    & + \\mathbb{E}_{\\mathbf{z} \\sim p_{z}(\\mathbf{z})}[(D(G(z)\\ |\\ E(\\tilde{x}))^2]\n\\end{split}\n\\end{align}\n\nFor the generator, we introduce two additional terms to encourage realistic and high-quality output. First, we add an $\\ell 1$ reconstruction term, scaled by a hyperparameter ($\\lambda_a = 100$ in our case). This is a common approach in image and audio settings. Second, we introduce a domain-specific term that performs an estimation of the ${T_{60}}$ values, the time it takes for the reverberation to decay by $60dB$, for the real and generated samples, and returns the absolute percent error between the two scaled by a hyperparameter ($\\lambda_b=100$ again). We term the differentiable ${T_{60}}$ proxy measure ${T_{60_p}}$. To compute this for log-spectrogram $x$, we first get the linear spectrogram $e^x$ and then sum along the time axis to obtain a fullband amplitude envelope. We use Schroeder's backward integration method to obtain a decay curve from the squared signal, and linearly extrapolate from the $-20dB$ point to get a ${T_{60}}$ estimate. In all:\n\n\\begin{align}\n\\begin{split}\n    \\underset{G}{\\text{min}}\\ V(G) &= \\mathbb{E}_{\\mathbf{z} \\sim p_{z}(\\mathbf{z})} \\left[ \\right. (1 - D(G(z)\\ |\\ E(\\tilde{x}))^2\\\\ \n     & + {\\lambda_a} \\left\\lVert G(z) - y \\right\\rVert {_1} \\\\\n     & + \\left. {\\lambda_b} \\left\\lvert \\dfrac{{T_{60_p}}(G(z)) - {T_{60_p}}(y)}{{T_{60_p}}(y)} \\right\\rvert \\right] \n\\end{split}\n\\end{align}\n\n\\paragraph{Training.}\nWe train our model on 8 NVIDIA 1080 Ti GPUs. Three Adam optimizers for each of the Generator, Discriminator, and Encoder were used to optimize the networks' parameter weights. Hyperparameters are noted in Table \\ref{tab:hyperparameters}. We make our model and code publicly available \\footnote{Model and code: \\href{https://github.com/nikhilsinghmus/image2reverb}{https://github.com/nikhilsinghmus/image2reverb}}.\n\n\\begin{table}[ht]\n    \\small\n    \\centering\n    \\begin{tabular}{c|c}\n        Parameter & Value\\\\\n        \\hline\n        ${\\eta_G}$ & 4e-4\\\\\n        ${\\eta_D}$ & 2e-4\\\\\n        ${\\eta_E}$ & 1e-5\\\\\n        $\\beta$ & (0.0, 0.99)\\\\\n        $\\epsilon$ & 1e-8\\\\\n    \\end{tabular}\n    \\caption{Hyperparameters for the Generator, Discriminator, and Encoder initial learning rates, the optimizer beta ($\\beta$), and epsilon ($\\epsilon$) for the Adam optimizers we use (one each for $D, G, E$)}\n    \\label{tab:hyperparameters}\n\\end{table}\n\n\\section{Results}\nUsing Image2Reverb we are able to generate perceptually plausible impulse responses for a diverse set of environments. In this section, we provide input-output examples to demonstrate the capabilities and applications of our model and also review results of a multi-stage evaluation integrating domain-specific quantitative metrics and expert ratings. Our goal is to examine output quality and conditional consistency, generally considered important for conditional GANs \\cite{devries2020on} and most relevant for our application.\n\n\\subsection{Examples}\nWe present several collections consisting of diverse examples in our supplementary material, with inputs curated to illustrate a range of settings of interest including famous spaces, musical environments, and entirely virtual spaces. All examples are made available as audiovisual collections\\footnote{Audiovisual samples: \\href{https://web.media.mit.edu/~nsingh1/image2reverb/}{https://web.media.mit.edu/\\raisebox{0.5ex}{\\texttildelow}nsingh1/image2reverb/}} and were generated with a model trained in around 12 hours, with 200 epochs on a virtual machine. Figure \\ref{fig:p_groundtruth} shows examples from our test set that were used in our expert evaluation (4 of 8, one from each category of: Small, Medium, Large, and Outdoor). We convolve a spoken word anechoic signal with the generated IRs for the reader to hear. Figure \\ref{fig:p_all} takes images of diverse scenes (art, animation, historical/recognizable places) as inputs. Figure \\ref{fig:p_vr} demonstrates how sections of 360-degree equirectangular images are cropped, projected, and passed through our model to generate IRs of spaces for immersive VR environments.\n\nWe strongly encourage the reader to explore these examples on the accompanying web page. We include examples of musical performance spaces, artistic depictions (drawings, paintings), 3D animation scenes, synthetic images from OpenAI's DALLâ€¢E, as well as real-world settings that present challenges (e.g. illusions painted on walls, reflections, etc.). These are largely created with real-world environments for which we may not have ground truth IRs, demonstrating how familiar and unusual scenes can be transformed in this way.\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{p_gt_ir_sig.png}\n    \\caption{Ground-truth measured IRs vs generated IRs. Columns show input images, depth maps, measured IRs with corresponding convolved speech, and generated IRs with corresponding convolved speech. Larger indoor spaces here tend to exhibit greater ${T_{60}}$ times with longer measured impulse responses. The outdoor scene has a very short measured IR and corresponding generated IR. Input images are all examples that were used in the expert survey and were drawn from the test set.}\n    \\label{fig:p_groundtruth}\n\\end{figure}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{p_all.png}\n    \\caption{Generated IR examples. Columns show input images, depth maps, generated IRs, and a dry anechoic speech signal before and after the generated IR was applied via convolution. Input images come from a variety of spaces which illustrate possible applications of our model. Some images are synthetic, including: an oil painting, a 3D animation still, and a video game screenshot. Others come from real-world scenes like a church (where music is often heard), a famous yet inaccessible space (SpaceX), and an outdoor desert scene. Larger indoor spaces  tend to exhibit longer impulse responses as seen here.}\n    \\label{fig:p_all}\n\\end{figure}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{p_vr.png}\n    \\caption{VR. Impulse responses generated from an equirectangular 360-degree image by sampling points on a sphere, cropping and applying a rectilinear projection to the resulting image, and feeding them into our model. This demonstrates how our model directly generates realistic impulse responses of panoramic virtual reality compatible images. Future work may allow generation of impulse responses using an entire 360-degree image, though at present there is a lack of paired data available for training.}\n    \\label{fig:p_vr}\n\\end{figure}\n\n\\subsection{Ablation Study}\nTo understand the contribution of key architectural components and decisions, we perform a study to characterize how removing each affects test set ${T_{60}}$ estimation after 50 training epochs. The three components are the depth maps, the ${T_{60_p}}$ objective term, and the pretrained Places365 weights for the ResNet50 encoder. Figure \\ref{fig:models_comparison} shows ${T_{60}}$ error distributions over the test set for each of these model variants, and Table \\ref{tab:modelcomparison_metrics} reports descriptive statistics.\n\nOur model reflects better mean error (closer to 0\\%) and less dispersion (a lower standard deviation) than the other variants. The former is well within the just noticeable difference (JND) bounds for ${T_{60}}$, often estimated as being around 25-30\\% for a musical signal \\cite{jndt60}. Additionally, this is an upper bound on authenticity: a more rigorous goal then perceptual plausibility \\cite{pellegrini2001quality}. The lower standard deviation indicates generally more consistent performance from this model across different examples, even in the presence of some that cause relatively large estimation errors due to incorrect interpretation of relevant qualities in the image, or inaccurate/noisy synthesis or estimation.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{t60_errord.png}\n    \\caption{${T_{60}}$ estimation error (\\%) distributions from each model version. ${T_{60}}$ estimates how long it takes the reverberation to decay by $60dB$. ``Main\" is our architecture as described earlier, ``No Depth\" omits the depth maps, ``No T60P\" omits the differentiable ${T_{60_p}}$ objective term, and ``No Places\" uses randomly initialized encoder weights. ``NN\" applies a nearest-neighbor approach with Places365-ResNet50 embeddings for images (errors clipped to 2000\\% for clarity). Descriptive statistics are given in Table \\ref{tab:modelcomparison_metrics}.}\n    \\label{fig:models_comparison}\n\\end{figure}\n\n\\begin{table}\n    \\small\n    \\centering\n    \\begin{tabular}{ccccccc}\n        & & Main & -Depth & -${T_{60_p}}$ & -P365 & NN\\\\\n        \\hline\n        & $\\mu$ & \\textbf{-6.03} & -9.17 & -7.1 & 43.15 & 149\\\\\n        & $\\sigma$ & \\textbf{78.8} & 83.1 & 85.97 & 144.3 & 491.02\\\\\\\\\n    \\end{tabular}\n    \\caption{${T_{60}}$ estimation error (\\%) statistics from each model version. ``Main\" is our architecture as described earlier, ``-Depth\" omits depth maps,``-${T_{60_p}}$\" omits the differentiable ${T_{60_p}}$ objective term, and ``-P365\" does not use the pretrained Places365 weights for the ResNet50 encoder. ``NN\" indicates a nearest-neighbor approach with Places365-ResNet50 embeddings for images. For mean and median, values closer to 0 reflect better performance. For the standard deviation, lower values reflect better performance. Distributions are visualized in Figure \\ref{fig:models_comparison}.}\n    \\label{tab:modelcomparison_metrics}\n\\end{table}\n\n\\subsection{Expert Evaluation}\nFollowing the finding that experienced acoustic engineers readily estimate a space's reverberant characteristics from an image \\cite{Kon2018}, we designed an experiment to evaluate our results. We note that this experiment is designed to estimate comparative perceptual plausibility, rather than (physical) authenticity (e.g. by side-by-side comparison to assess whether any difference can be heard). These goals have been differentiated in prior work \\cite{pellegrini2001quality}. We selected two arbitrary examples from each of the four scene categories and recruited a panel of 31 experts, defined as those with significant audio experience, to participate in a within-subjects study. For each of these examples, we convolved an arbitrary anechoic signal with the output IR, as well as the ground truth IR. These 16 samples were presented in randomized order and participants were instructed to rate each on a scale from 1 to 5 based on 1) reverberation quality, and 2) realism or ``match\" between their expected reverb based on the image and the presented signal with reverb applied. Participants answered one reverb-related screening question to demonstrate eligibility, and two attention check questions at the end of the survey. The four scene categories are: Large, Medium, Outdoor, and Small. These demonstrate diversity in visual-reverb relationships. The dependent variables are quality and match ratings, and the independent variables are IR source (real vs. fake) and scene category (the four options listed previously). We first test our data for normality with D'Agostino and Pearson's omnibus test \\cite{pearson1977tests}, which indicates that our data is statistically normal ($p>.05$).\n\nA two-way repeated-measures ANOVA revealed a statistically significant interaction between IR source and scene category for both quality ratings, $F(3, 90)=7.04$, $p\\leq .001$, and match ratings, $F(3, 90)=3.73$, $p=.02$ (reported $p$-values are adjusted with the Greenhouse-Geisser correction \\cite{greenhouse1959methods}). This indicates that statistically significant differences between ratings for real and fake IR reverbs depend on the scene category. Per-participant ratings and rating changes, overall and by scene, are shown in Figure \\ref{fig:ratings}.\n\nSubsequent tests for simple main effects with paired two one-sided tests indicate that real vs. fake ratings are statistically equivalent ($p<.05$) for large and small quality ratings, and large, medium, and small match ratings. These tests are carried out with an $\\epsilon$ of 1 (testing for whether the means of the two populations differ by at least 1). Results are shown in Table \\ref{tab:ratings_maineffects}. Notably, outdoor scenes appear to contribute to the rating differences between real and fake IRs. We conjecture this is due to outdoor scenes being too different a regime from the vast majority of our data, which are indoor, to model effectively. Additionally, medium-sized scenes appear to contribute to differences in quality.\n\n\\begin{table}\n    \\small\n    \\centering\n    \\begin{tabular}{llcc}\n        Rating & Scene & DoF & $p$\\\\\n        \\hline\n        Quality & Large & 56 & \\textbf{$<.001$} \\\\\n        Quality & Medium & 56 & $.28$ \\\\\n        Quality & Outdoor & 56 & $.62$ \\\\\n        Quality & Small & 56 & \\textbf{$<.001$} \\\\\n        Match & Large & 56 & \\textbf{$<.001$} \\\\\n        Match & Medium & 56 & \\textbf{$.006$} \\\\\n        Match & Outdoor & 56 & $.29$ \\\\\n        Match & Small & 56 & \\textbf{$<.05$} \\\\\n        \n    \\end{tabular}\n    \\caption{Simple main effect tests for equivalence between real and generated IRs across different categories of scenes. We use paired two one-sided tests with bounds ($\\epsilon$) of 1 and Bonferroni-adjusted p-values. These results suggest that real vs. fake ratings are statistically equivalent within one rating unit (the resolution of the rating scale) for large and small quality ratings, and large, medium, and small match ratings. Notably, outdoor scenes contribute to the difference between real and fake IRs and medium-sized scenes contribute to differences in quality.}\n    \\label{tab:ratings_maineffects}\n\\end{table}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{scene_ratings.png}\n    \\caption{Expert evaluation results. Paired plots showing per-participant quality and match differences in rating for each scene category. Green lines indicate higher rating for real IRs, red lines for generated IRs, and grey lines equivalent ratings for both.}\n    \\label{fig:ratings}\n\\end{figure}\n\n\\subsection{Model Behavior and Interpretation}\n\n\\paragraph{Effect of varying depth.}\nWe compare the full estimated depth map with constant depth maps filled with either 0 or 0.5 (chosen based on the approximate lower and upper bounds of our data). We survey the distributions of generated IRs' ${T_{60}}$ values over our test set, the results of which are shown in Figure \\ref{fig:t60_distributions}. Table \\ref{tab:t60_distributions} reports descriptive statistics for these distributions, showing that the main model's output IRs' decay times are biased lower by the 0-depth input and higher by the 0.5-depth input respectively. These may indicate some potential for steering the model in interactive settings. We do note, however, that behavior with constant depth values greater than 0.5 is less predictable. This may be due to the presence of outdoor scenes, for which the scene's depth may not be correlated with IR duration. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{t60_kde.png}\n    \\caption{Effect of Depth on ${T_{60}}$. Distributions of estimated ${T_{60}}$ values for the model with estimated depth maps, plus constant depth maps set to either 0 (low) or 0.5 (high). Manipulating the depth value allows us to ``suggest\" smaller or larger scenes, i.e. bias the output of the model. Table \\ref{tab:t60_distributions} shows corresponding descriptive statistics. These results indicate a level of ``steerability\" for the model's behavior in human-in-the-loop settings.}\n    \\label{fig:t60_distributions}\n\\end{figure}\n\n\\begin{table}\n    \\small\n    \\centering\n    \\begin{tabular}{cccc}\n        & Main & Depth 0 & Depth 0.5\\\\\n        \\hline\n        $\\mu$ & 2.07 & 2.01 & 3.62\\\\\n        $\\sigma$ & 1.54 & 0.87 & 2.36\\\\\n        Mdn. & 2.69 & 2.00 & 3.07\\\\\n    \\end{tabular}\n    \\caption{Descriptive statistics for the model with estimated depth maps, as well as constant depth maps set to either 0 or 0.5. The full depth map's results are between that of the 0 and 0.5 depth maps. Figure \\ref{fig:t60_distributions} visulizes the corresponding distributions.}\n    \\label{tab:t60_distributions}\n\\end{table}\n\n\\paragraph{Effect of transfer learning.}\nTo understand which visual features are important to our encoder, we use Gradient-weighted Class Activation Mapping (Grad-CAM) \\cite{selvaraju2017grad}. Grad-CAM is a popularly applied strategy for visually interpreting convolutional neural networks by localizing important regions contributing to a given target feature (or class in a classification setting). We produce such maps for our test images with both the ResNet50 pre-trained on Places365 dataset, as well as the final encoder model. All resulting pairs exhibit noticeable differences; we check for this with the structural similarity index (SSIM) metric \\cite{wang2004image}, which is below 0.98 for all examples.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{gradcam_reflective.png}\n    \\caption{Grad-CAMs for images passed through the pre-trained Places365 ResNet50 encoder vs. our fine-tuned encoder, showing movement towards significant reflective areas for \\textbf{(A)} a small, and \\textbf{(B)} a large environment. The fine-tuned model's activations highlight larger reflective surfaces: depth of staircase for \\textbf{(A)} vs. railing that may be more optimal for scene identification, and wall-to-ceiling corner plus surrounding areas for (B).}\n    \\label{fig:gradcam_reflective}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{gradcam_textured.png}\n    \\caption{Grad-CAMs for images passed through both the pre-trained Places365 ResNet50 encoder and our fine-tuned encoder, showing movement towards more textured areas for \\textbf{(A)} an indoor, and \\textbf{(B)} an outdoor environment. The former seems to contain significant absorption and the latte has few reflective surfaces. In both cases, textured areas are highlighted. These may be associated with absorption, diffusion, and more sparse reflections depending on the scene.}\n    \\label{fig:gradcam_textured}\n\\end{figure}\n\nWe qualitatively survey these and identify two broad change regimes, which are illustrated with particular examples. First, we observe that the greatest-valued feature is often associated with activations of visual regions corresponding to large reflective surfaces. Examples are shown in Figure \\ref{fig:gradcam_reflective}. Often, these are walls ceilings, windows, and other surfaces in reflective environments. Second, we find that textured areas are highlighted in less reflective environments. Examples of these are shown in Figure \\ref{fig:gradcam_textured}. These may correspond to sparser reflections and diffusion.\n\n\\paragraph{Limitations and future work.}\nMany images of spaces may offer inaccurate portrayals of the relevant properties (size, shape, materials, etc.), or may be misleading (examples in supplementary material), leading to erroneous estimations. Our dataset also contains much variation in other relevant parameters (e.g. $DRR$ and $EDT$) in a way we cannot semantically connect to paired images, given the sources of our data. New audio IR datasets collected with strongly corresponding photos may allow us to effectively model these characteristics precisely.\n\n\\section{Conclusion}\nWe introduced Image2Reverb, a system that is able to directly synthesize audio impulse responses from single images. These are directly applied in downstream convolution reverb settings to simulate depicted environments, with applications to XR, music production, television and film post-production, video games, videoconferencing, and other media. Our quantitative and human-expert evaluation shows significant strengths, and we discuss the method's limitations. We demonstrate that end-to-end image-based synthesis of plausible audio impulse responses is feasible, given such diverse applications. We hope our results provide a helpful benchmark for the community and future work and inspire creative applications.\n\n{\\small\n\\paragraph{Acknowledgements}\nWe thank the reviewers for their thorough feedback and useful suggestions. Additionally, we thank James Traer, Dor Verbin, and Phillip Isola for helpful discussions. We thank Google for a cloud platform education grant.\n}\n\n{\\small\n\\bibliographystyle{ieee_fullname}\n\\bibliography{main}\n}\n\n\\end{document}\n",j=n(4376),k=n(109),_=n(8642),R=n(5078),T=n(2870);n(2702);let I=e=>{let{node:t,bib:n}=e,i=t.args[1].content[0].content.toLowerCase();if(!n[i])return;let s=n[i].i,o=n[i].entry;return(0,a.jsxs)(_.z,{children:[(0,a.jsx)(_.z.Target,{children:(0,a.jsx)(c.x,{display:"inline",children:(0,a.jsx)(l.e,{href:"#ref:"+s,children:"["+s+"]"})})}),(0,a.jsx)(_.z.Dropdown,{children:(0,a.jsxs)(h.x,{p:"md",style:{width:400},children:[(0,a.jsx)(r.D,{order:6,children:(0,a.jsx)("b",{children:(0,j.normalizeFieldValue)(o.fields.title)})}),(0,a.jsx)(c.x,{size:"sm",children:(0,a.jsx)("b",{children:(0,j.normalizeFieldValue)(o.fields.author)+"."})}),(0,a.jsx)(c.x,{size:"sm",children:(0,a.jsx)("b",{children:(0,j.normalizeFieldValue)(o.fields.year)})})]})})]})},z=e=>{let{node:t,tables:n,render:i}=e,r=t.content.filter(e=>"tabular"===e.env)[0],s=r.args[1].content[0].content.replace("|","").length,o=r.content.map((e,t)=>"string"===e.type||"\\"===e.content?e.content:i(e,t)).filter(e=>e&&" "!==e),l=[];var u=0;let p=t.content.filter(e=>"label"===e.content)[0].args.filter(e=>"{"===e.openMark)[0].content.map(e=>e.content).join(""),m=t.content.filter(e=>"caption"===e.content)[0].args.filter(e=>"{"===e.openMark)[0].content.map(i);for(let e=0;e<o.length;e+=1)if(["&","\\"].includes(o[e])){let t=o.slice(u,e).map((e,t)=>e.content?i(e,t):e);l=[...l,t],u=e+1}l=[...l,o.slice(u,o.length).map((e,t)=>e.content?i(e,t):e)];let g=[];for(let e=0;e<l.length;e+=s){let t=l.slice(e,e+s);g=[...g,t]}return(0,a.jsx)(d.M,{id:p,children:(0,a.jsxs)(h.x,{pb:"xl",pt:"sm",style:{maxWidth:"table*"===t.env?"100%":"50%"},children:[(0,a.jsxs)(c.x,{size:"sm",color:"dimmed",align:"justify",children:[(0,a.jsxs)("b",{children:["Table ",n[p],". "]}),m]}),(0,a.jsxs)(R.i,{pt:"xl",children:[(0,a.jsx)("thead",{children:(0,a.jsx)("tr",{children:g[0].map((e,t)=>(0,a.jsx)("th",{children:e},t))})}),(0,a.jsx)("tbody",{children:g.slice(1).map((e,t)=>(0,a.jsx)("tr",{children:e.map((e,t)=>(0,a.jsx)("td",{children:e},t))},t))})]})]})})},M=e=>{let{node:t,figures:n,render:i}=e,r=t.content.filter(e=>"includegraphics"===e.content)[0].args.filter(e=>"{"===e.openMark)[0].content.map(e=>e.content).join(""),s=t.content.filter(e=>"includegraphics"===e.content)[0].args.filter(e=>"["===e.openMark)[0].content,o=4==s.length?+s[2].content:1,l=t.content.filter(e=>"caption"===e.content)[0].args.filter(e=>"{"===e.openMark)[0].content.map(i),u=t.content.filter(e=>"label"===e.content)[0].args.filter(e=>"{"===e.openMark)[0].content.map(e=>e.content).join("");return(0,a.jsx)(d.M,{id:u,children:(0,a.jsxs)(h.x,{pb:"xl",pt:"sm",style:{maxWidth:"calc(".concat("figure*"===t.env?"100%":"50%"," * ").concat(o,")")},children:[(0,a.jsx)(T.E,{src:"https://web.media.mit.edu/~nsingh1/files/tex/"+r,alt:r}),(0,a.jsxs)(c.x,{size:"sm",color:"dimmed",align:"justify",children:[(0,a.jsxs)("b",{children:["Figure ",n[u],". "]}),l]})]})})},A=e=>{let{node:t,figures:n,render:i}=e,r=t.content.filter(e=>"includegraphics"===e.content)[0].args.filter(e=>"{"===e.openMark)[0].content.map(e=>e.content).join(""),s=t.content[6].content.map(i),o=t.content.filter(e=>"label"===e.content)[0].args.filter(e=>"{"===e.openMark)[0].content.map(e=>e.content).join("");return(0,a.jsx)(d.M,{id:o,children:(0,a.jsxs)(h.x,{pb:"xl",pt:"sm",style:{maxWidth:"100%"},children:[(0,a.jsx)(T.E,{src:"https://web.media.mit.edu/~nsingh1/files/tex/"+r,alt:r}),(0,a.jsxs)(c.x,{size:"sm",color:"dimmed",align:"justify",children:[(0,a.jsxs)("b",{children:["Figure ",n[o],". "]}),s]})]})})},D=e=>{let{node:t,algorithms:n,render:i}=e,r=t.content.filter(e=>"caption"===e.content)[0].args.filter(e=>"{"===e.openMark)[0].content.map(i),s=t.content.filter(e=>"label"===e.content)[0].args.filter(e=>"{"===e.openMark)[0].content.map(e=>e.content).join("");return(0,a.jsx)(d.M,{children:(0,a.jsxs)(h.x,{style:{maxWidth:800,backgroundColor:"rgba(120, 120, 120, 0.2)",padding:10,marginBottom:20,borderRadius:8},children:[(0,a.jsxs)(c.x,{size:"sm",color:"dimmed",align:"justify",children:[(0,a.jsxs)("b",{children:["Algorithm ",n[s],". "]}),r]}),(0,a.jsx)("br",{}),(0,a.jsx)(c.x,{size:"sm",children:t.content.map(i)})]})})},E=()=>{let[e,t]=(0,i.useState)([]),[p,m]=(0,i.useState)({}),g=(0,i.useMemo)(()=>Object.fromEntries([...$.matchAll(/\\label{fig:([^}]+)}/g)].map((e,t)=>["fig:"+e[1],t+1])),[]),f=(0,i.useMemo)(()=>Object.fromEntries([...$.matchAll(/\\label{tab:([^}]+)}/g)].map((e,t)=>["tab:"+e[1],t+1])),[]),b=(0,i.useMemo)(()=>Object.fromEntries([...$.matchAll(/\\label{algo:([^}]+)}/g)].map((e,t)=>["algo:"+e[1],t+1])),[]),w=(0,i.useCallback)((e,t)=>{switch(e.type){case"macro":switch(e.content){case"title":return(0,a.jsx)(r.D,{align:"center",pt:"lg",pb:"xl",order:1,children:e.args[e.args.length-1].content.map(w)},t);case"author":let n=[];return e.args[0].content.forEach((e,t)=>{(0==t||"and"==e.content)&&n.push([]),n[n.length-1].push(e)}),(0,a.jsx)(s.Z,{pb:"xl",grow:!0,children:n.map((e,t)=>(0,a.jsx)(r.D,{align:"center",order:5,children:e.map(w)},t))},t);case"section":return(0,a.jsx)(r.D,{pt:"lg",pb:0,order:2,children:e.args[e.args.length-1].content.map(w)},t);case"subsection":return(0,a.jsx)(r.D,{pt:"lg",pb:0,order:4,children:e.args[e.args.length-1].content.map(w)},t);case"subsubsection":case"paragraph":return(0,a.jsx)(r.D,{pt:"lg",pb:0,order:5,children:e.args[e.args.length-1].content.map(w)},t);case"textit":return(0,a.jsx)("i",{children:e.args[e.args.length-1].content.map(w)},t);case"textbf":return(0,a.jsx)("b",{children:e.args[e.args.length-1].content.map(w)},t);case"textsc":return(0,a.jsx)("span",{style:{fontVariant:"small-caps"},children:e.args[e.args.length-1].content.map(w)},t);case"medskip":return(0,a.jsx)(o.T,{h:"md"},t);case"smallskip":o.T;case"\\":return(0,a.jsx)("br",{},t);case"&":return(0,a.jsx)("span",{children:"&"},t);case"^":return(0,a.jsx)("sup",{children:e.args[0].content.map(w)},t);case"href":if(e.args.length<2)return e.args[0].content;return(0,a.jsx)(l.e,{href:e.args[e.args.length-2].content.map(e=>e.content).join(""),children:e.args[e.args.length-1].content.map(w)},t);case"label":let i=e.args[e.args.length-1].content.map(e=>e.content).join("");return(0,a.jsx)("span",{id:i},t);case"KwIn":return(0,a.jsxs)(r.D,{order:6,children:[(0,a.jsx)("br",{}),"In:"]},t);case"KwOut":return(0,a.jsxs)(r.D,{order:6,children:[(0,a.jsx)("br",{}),"Out:"]},t);case"For":return(0,a.jsxs)(r.D,{display:"inline",order:6,children:[(0,a.jsx)("br",{}),"For "]},t);case"cite":return(0,a.jsx)(I,{node:e,bib:p},t);case"ref":let u=e.args[e.args.length-1].content.map(e=>e.content).join(""),m=u.split(":")[0],v={fig:g,tab:f}[m];if(v)return(0,a.jsx)(l.e,{href:"#"+u,children:v[u]},t);default:return}case"string":return(0,a.jsx)(c.x,{display:"inline",children:e.content},t);case"whitespace":return" ";case"group":return e.content.map(w);case"parbreak":case"comment":return null;case"inlinemath":return(0,a.jsx)(k.Z,{inline:!0,children:"\\("+(0,x.r)(e).replace(/\$/g,"")+"\\)"},t);case"environment":switch(e.env){case"itemize":return(0,a.jsx)("ul",{children:e.args.map((e,t)=>(0,a.jsx)("li",{children:e.content.map(w)},t))},t);case"enumerate":return(0,a.jsx)("ol",{children:e.args.map((e,t)=>(0,a.jsx)("li",{children:e.content.map(w)},t))},t);case"figure":case"figure*":return(0,a.jsx)(M,{node:e,figures:g,render:w},t);case"strip":return(0,a.jsx)(A,{node:e,figures:g,render:w},t);case"table":case"table*":return(0,a.jsx)(z,{node:e,tables:f,render:w},t);case"algorithm":return(0,a.jsx)(D,{node:e,algorithms:b,render:w},t);case"abstract":return(0,a.jsx)(d.M,{children:(0,a.jsx)(h.x,{pt:"xl",pb:"xl",style:{maxWidth:"90%"},children:(0,a.jsxs)(c.x,{size:"sm",children:[(0,a.jsx)("b",{children:"Abstract. "}),e.content.map(w)]})})},t);default:console.log(e);return}case"mathenv":return(0,a.jsx)(d.M,{children:(0,a.jsx)(h.x,{style:{maxWidth:800},children:(0,a.jsx)(k.Z,{children:(0,x.r)(e.content[0])})})},t);default:console.log(e);return}},[p,b,g,f]);return((0,i.useEffect)(()=>{y($).then(e=>t((null==e?void 0:e.content.map(w))||[]))},[w]),(0,i.useEffect)(()=>{let e=$.matchAll(/\\bibliography{(.*)}/g).next().value[1];n(6083)("./".concat(e,".bib")).then(e=>{let t=(0,j.parseBibFile)(e.default).entries$,n=Object.fromEntries(Object.entries(t).map((e,t)=>{let[n,a]=e;return[n,{entry:a,i:t+1}]}));m(n)})},[]),p&&e&&Object.keys(p).length&&Object.keys(e).length)?(0,a.jsxs)(a.Fragment,{children:[e,(0,a.jsx)("br",{}),(0,a.jsx)("br",{}),(0,a.jsx)(r.D,{order:2,children:"References"}),Object.values(p).map(e=>(0,a.jsxs)(c.x,{id:"ref:"+e.i,size:"sm",color:"dimmed",align:"justify",children:[(0,a.jsx)("b",{children:"["+e.i+"]"}),(0,a.jsx)("i",{children:(0,j.normalizeFieldValue)(e.entry.fields.title)}),". "+(0,j.normalizeFieldValue)(e.entry.fields.author)+" "+(0,j.normalizeFieldValue)(e.entry.fields.year)+"."]},e.i))]}):(0,a.jsx)(d.M,{pt:400,children:(0,a.jsx)(u.a,{sx:e=>({color:e.colors[e.primaryColor][6]})})})};var N=!0,G=E}},function(e){e.O(0,[587,978,231,928,645,774,888,179],function(){return e(e.s=9918)}),_N_E=e.O()}]);